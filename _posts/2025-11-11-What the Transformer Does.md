---
layout: post
title: What the Transformer Does?
---
A Transformer is like a smart classroom.  
Every word in a sentence is a student sitting in class.  
Each student can look around at others and decide who to listen to before writing down their own understanding.  
That â€œlooking around and deciding who to listen toâ€ part is called: Seft-attention  
Embedding: Turning Words into Numbers  
Before words can talk to each other, they must become numbers.  
The computer canâ€™t understand â€œcatâ€ or â€œdogâ€, but it can understand something like:  
| Word | Embedding (tiny example) |
| ---- | ------------------------ |
| cat  | [0.2, 0.8]               |
| dog  | [0.3, 0.7]               |
| car  | [0.9, 0.1]               |
These are just coordinates â€” like placing â€œcatâ€ and â€œdogâ€ close to each other on a map because they mean similar things.

ğŸ§  Why it matters:
Embeddings give every word a shape of meaning.

ğŸ² Kid analogy:
Every toy has a color and size. â€œCatâ€ and â€œdogâ€ are both small and fluffy; â€œcarâ€ is big and shiny. The colors and sizes are like embedding numbers.
