---
layout: post
title: What the Transformer Does?
---
The Building Blocks: What Powers an LLM?  
An LLM isn't a single "brain"—it's a stack of layers called Transformers. Each transformer layer processes text in parallel (unlike older models like RNNs that go sequentially). Key parts:Tokens: Text is chopped into small pieces (e.g., words or subwords). "The cat ate fish" → Tokens: ["The", "cat", "ate", "fish"].
Embeddings: Each token becomes a vector (a list of numbers, like [0.1, 0.5, -0.2, ...]) representing its meaning. Learned during training.
Positional Encoding: Adds info about word order (e.g., "cat ate" ≠ "ate cat") using math waves.
Self-Attention: The star of the show—lets words "talk" to each other to understand context.
Feed-Forward Network (FFN): A simple neural net per word to refine representations.
Layer Norm & Residual Connections: Stabilize training by adding "shortcuts."

The model has many layers (e.g., 96 in GPT-4) stacked, each building richer understanding.
